{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3985472e",
   "metadata": {},
   "source": [
    "# Gemini Batch API Test Notebook\n",
    "\n",
    "This notebook demonstrates how to use the Gemini Batch API for processing large volumes of rug analysis requests asynchronously at 50% reduced cost. The Batch API is ideal for non-urgent, large-scale tasks with a target turnaround time of 24 hours.\n",
    "\n",
    "## Features Covered:\n",
    "- File-based batch processing with JSONL uploads\n",
    "- Inline batch requests for smaller datasets  \n",
    "- Job status monitoring and result retrieval\n",
    "- Error handling and job management\n",
    "- Integration with our Next.js rug processing application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89a7ab",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries\n",
    "\n",
    "Import all necessary libraries for working with the Gemini Batch API, including authentication, file handling, and JSON processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eee66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install google-generativeai python-dotenv requests\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import Gemini SDK\n",
    "try:\n",
    "    from google import genai\n",
    "    from google.genai import types\n",
    "    print(\"‚úÖ Google GenAI SDK imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå Error importing Google GenAI SDK:\", e)\n",
    "    print(\"üí° Install with: pip install google-generativeai\")\n",
    "\n",
    "# Standard libraries for API calls and file handling\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b16afa",
   "metadata": {},
   "source": [
    "## 2. Configure Gemini API Client\n",
    "\n",
    "Set up the Gemini API client with authentication and initialize for batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7175bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API credentials\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY') or os.getenv('GOOGLE_GENERATIVE_AI_API_KEY')\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"‚ùå No API key found!\")\n",
    "    print(\"üí° Set GEMINI_API_KEY or GOOGLE_GENERATIVE_AI_API_KEY environment variable\")\n",
    "    GEMINI_API_KEY = input(\"Enter your Gemini API key: \")\n",
    "\n",
    "# Initialize Gemini client\n",
    "try:\n",
    "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    print(\"‚úÖ Gemini client initialized successfully\")\n",
    "    \n",
    "    # Test API connection with a simple request\n",
    "    print(\"üîç Testing API connection...\")\n",
    "    # Note: You would add a simple API test here if needed\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing client: {e}\")\n",
    "\n",
    "# Configuration for our rug processing batch jobs\n",
    "BATCH_CONFIG = {\n",
    "    'model': 'gemini-2.5-flash',\n",
    "    'base_url': 'https://generativelanguage.googleapis.com/v1beta',\n",
    "    'default_display_name': 'Rug Analysis Batch Job',\n",
    "    'max_file_size': 2 * 1024 * 1024 * 1024,  # 2GB limit\n",
    "    'poll_interval': 30  # seconds\n",
    "}\n",
    "\n",
    "print(f\"üìã Batch configuration loaded:\")\n",
    "print(f\"   Model: {BATCH_CONFIG['model']}\")\n",
    "print(f\"   Max file size: {BATCH_CONFIG['max_file_size'] / (1024**3):.1f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadff358",
   "metadata": {},
   "source": [
    "## 3. Create JSONL File for Batch Requests\n",
    "\n",
    "Generate a JSON Lines file containing multiple rug analysis requests. Each line contains a complete `GenerateContentRequest` object with a unique key for result mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample rug data for testing (simulating CSV data from our Next.js app)\n",
    "sample_rugs = [\n",
    "    {\n",
    "        \"sku\": \"26171\",\n",
    "        \"title\": \"3'2\\\"x13'10\\\" Antique Persian Northwest Boteh Design Runner Handmade Rug 26171\",\n",
    "        \"size\": \"3'2\\\" x 13'10\\\"\",\n",
    "        \"material\": \"Wool\",\n",
    "        \"origin\": \"IRAN (Islamic Republic of Iran)\",\n",
    "        \"style\": \"Traditional\",\n",
    "        \"prompt\": \"Photo-realistic hallway in Traditional Decor, featuring an indoor rug, with natural daylight. Place a runner area rug (3'2\\\" x 13'10\\\") centered under a coffee table. Rug collection: Persian; secondary: Persian,Antique; style: Traditional; origin: IRAN (Islamic Republic of Iran). Pile: Wool; foundation: Wool; material: Wool; weave type: Hand-Knotted; dominant colors: Blue, Ivory, Navy Blue, Blue, Pink, Gold, Forest Green, Purple, Black, Denim Blue. Preserve the rug's real physical proportions exactly as shown in the product image. Maintain the correct length-to-width ratio with no distortion, stretching, compression, or reshaping. Render the rug in the scene at a realistic scale relative to the room and surrounding objects. Hardwood floor, soft shadows, realistic perspective from eye level (~1.2m), 35mm lens, high detail.\"\n",
    "    },\n",
    "    {\n",
    "        \"sku\": \"28392\",\n",
    "        \"title\": \"8'x10' Modern Contemporary Abstract Area Rug\",\n",
    "        \"size\": \"8' x 10'\",\n",
    "        \"material\": \"Polypropylene\",\n",
    "        \"origin\": \"Turkey\",\n",
    "        \"style\": \"Modern\",\n",
    "        \"prompt\": \"Photo-realistic living room in Modern Decor, featuring an indoor rug, with natural daylight. Place a rectangle area rug (8' x 10') centered under a coffee table. Rug collection: Modern & Contemporary; secondary: Abstract; style: Modern; origin: Turkey. Pile: Low pile; foundation: Jute; material: Polypropylene; weave type: Machine-made; dominant colors: Grey, White, Black, Silver. Preserve the rug's real physical proportions exactly as shown in the product image. Maintain the correct length-to-width ratio with no distortion, stretching, compression, or reshaping. Render the rug in the scene at a realistic scale relative to the room and surrounding objects. Hardwood floor, soft shadows, realistic perspective from eye level (~1.2m), 35mm lens, high detail.\"\n",
    "    },\n",
    "    {\n",
    "        \"sku\": \"31045\",\n",
    "        \"title\": \"5'x8' Vintage Distressed Oriental Area Rug\",\n",
    "        \"size\": \"5' x 8'\",\n",
    "        \"material\": \"Wool\",\n",
    "        \"origin\": \"India\",\n",
    "        \"style\": \"Transitional\",\n",
    "        \"prompt\": \"Photo-realistic living room, parlor or library in Transitional Decor, featuring an indoor rug, with natural daylight. Place a rectangle area rug (5' x 8') centered under a coffee table. Rug collection: Vintage; secondary: Distressed; style: Transitional; origin: India. Pile: Medium; foundation: Cotton; material: Wool; weave type: Hand-tufted; dominant colors: Beige, Brown, Rust, Cream. Preserve the rug's real physical proportions exactly as shown in the product image. Maintain the correct length-to-width ratio with no distortion, stretching, compression, or reshaping. Render the rug in the scene at a realistic scale relative to the room and surrounding objects. Hardwood floor, soft shadows, realistic perspective from eye level (~1.2m), 35mm lens, high detail.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def create_batch_requests(rugs_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create batch requests in the format expected by Gemini Batch API\n",
    "    \"\"\"\n",
    "    batch_requests = []\n",
    "    \n",
    "    for rug in rugs_data:\n",
    "        request = {\n",
    "            \"key\": f\"rug-{rug['sku']}\",\n",
    "            \"request\": {\n",
    "                \"contents\": [{\n",
    "                    \"parts\": [{\n",
    "                        \"text\": f\"\"\"Analyze this rug and generate a detailed product description based on the following prompt: {rug['prompt']}\n",
    "\n",
    "Please provide:\n",
    "1. A detailed visual description of the rug\n",
    "2. Suggested room placement and styling tips  \n",
    "3. Key features and benefits\n",
    "4. Care instructions\n",
    "\n",
    "Product Details:\n",
    "- SKU: {rug['sku']}\n",
    "- Title: {rug['title']}\n",
    "- Size: {rug['size']}\n",
    "- Material: {rug['material']}\n",
    "- Origin: {rug['origin']}\n",
    "- Style: {rug['style']}\"\"\"\n",
    "                    }]\n",
    "                }],\n",
    "                \"generation_config\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_output_tokens\": 1000\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        batch_requests.append(request)\n",
    "    \n",
    "    return batch_requests\n",
    "\n",
    "# Generate batch requests\n",
    "print(\"üìù Creating batch requests...\")\n",
    "batch_requests = create_batch_requests(sample_rugs)\n",
    "print(f\"‚úÖ Created {len(batch_requests)} batch requests\")\n",
    "\n",
    "# Create JSONL file\n",
    "jsonl_filename = \"rug-batch-requests.jsonl\"\n",
    "jsonl_path = Path(jsonl_filename)\n",
    "\n",
    "print(f\"üíæ Writing JSONL file: {jsonl_filename}\")\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "    for request in batch_requests:\n",
    "        f.write(json.dumps(request) + '\\n')\n",
    "\n",
    "file_size = jsonl_path.stat().st_size\n",
    "print(f\"‚úÖ JSONL file created successfully\")\n",
    "print(f\"   File size: {file_size:,} bytes ({file_size / 1024:.1f} KB)\")\n",
    "print(f\"   Requests: {len(batch_requests)}\")\n",
    "\n",
    "# Display first request as example\n",
    "print(f\"\\nüìã Sample request (first item):\")\n",
    "print(json.dumps(batch_requests[0], indent=2)[:500] + \"...\" if len(json.dumps(batch_requests[0], indent=2)) > 500 else json.dumps(batch_requests[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a75671",
   "metadata": {},
   "source": [
    "## 4. Upload File Using File API\n",
    "\n",
    "Upload the JSONL file to Google's File API to prepare for batch job creation. Files are automatically deleted after 2 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload JSONL file using Gemini File API\n",
    "def upload_file_to_gemini(file_path: Path, display_name: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Upload a file to Gemini File API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üì§ Uploading file: {file_path}\")\n",
    "        \n",
    "        # Use the Gemini client to upload\n",
    "        uploaded_file = client.files.upload(\n",
    "            file=str(file_path),\n",
    "            config=types.UploadFileConfig(\n",
    "                display_name=display_name,\n",
    "                mime_type='application/jsonl'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ File uploaded successfully!\")\n",
    "        print(f\"   File name: {uploaded_file.name}\")\n",
    "        print(f\"   Display name: {uploaded_file.display_name}\")\n",
    "        print(f\"   Size: {uploaded_file.size_bytes:,} bytes\")\n",
    "        print(f\"   State: {uploaded_file.state}\")\n",
    "        print(f\"   Expires: {uploaded_file.expiration_time}\")\n",
    "        \n",
    "        return {\n",
    "            'name': uploaded_file.name,\n",
    "            'display_name': uploaded_file.display_name,\n",
    "            'size_bytes': uploaded_file.size_bytes,\n",
    "            'state': uploaded_file.state,\n",
    "            'uri': uploaded_file.uri,\n",
    "            'expiration_time': uploaded_file.expiration_time\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Upload the JSONL file\n",
    "display_name = f\"Rug Batch Requests {time.strftime('%Y-%m-%d %H:%M')}\"\n",
    "uploaded_file_info = upload_file_to_gemini(jsonl_path, display_name)\n",
    "\n",
    "if uploaded_file_info:\n",
    "    print(f\"\\nüéâ File upload successful!\")\n",
    "    print(f\"üìÅ File reference: {uploaded_file_info['name']}\")\n",
    "else:\n",
    "    print(\"‚ùå File upload failed. Cannot proceed with batch job creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570b0e3",
   "metadata": {},
   "source": [
    "## 5. Submit Batch Job\n",
    "\n",
    "Create a batch job using the uploaded file. This initiates asynchronous processing at 50% cost reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch job using the uploaded file\n",
    "def create_batch_job(file_name: str, display_name: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create a batch job using an uploaded file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üöÄ Creating batch job...\")\n",
    "        \n",
    "        batch_job = client.batches.create(\n",
    "            model=BATCH_CONFIG['model'],\n",
    "            src=file_name,  # Use the file name from upload\n",
    "            config={\n",
    "                'display_name': display_name\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Batch job created successfully!\")\n",
    "        print(f\"   Job name: {batch_job.name}\")\n",
    "        print(f\"   Display name: {batch_job.display_name}\")\n",
    "        print(f\"   State: {batch_job.state}\")\n",
    "        print(f\"   Model: {batch_job.model}\")\n",
    "        print(f\"   Created: {batch_job.create_time}\")\n",
    "        \n",
    "        return {\n",
    "            'name': batch_job.name,\n",
    "            'display_name': batch_job.display_name,\n",
    "            'state': batch_job.state,\n",
    "            'model': batch_job.model,\n",
    "            'create_time': batch_job.create_time,\n",
    "            'full_job': batch_job\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating batch job: {e}\")\n",
    "        return None\n",
    "\n",
    "# Only proceed if file upload was successful\n",
    "batch_job_info = None\n",
    "if uploaded_file_info:\n",
    "    job_display_name = f\"Rug Analysis Batch - {time.strftime('%Y-%m-%d %H:%M')}\"\n",
    "    batch_job_info = create_batch_job(uploaded_file_info['name'], job_display_name)\n",
    "    \n",
    "    if batch_job_info:\n",
    "        print(f\"\\nüéØ Batch job ready for monitoring!\")\n",
    "        print(f\"üìã Job ID: {batch_job_info['name']}\")\n",
    "        \n",
    "        # Store job name for monitoring\n",
    "        current_job_name = batch_job_info['name']\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create batch job\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping batch job creation - no uploaded file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98d568",
   "metadata": {},
   "source": [
    "## 6. Monitor Job Status\n",
    "\n",
    "Implement polling logic to monitor batch job progress through different states: PENDING ‚Üí RUNNING ‚Üí SUCCEEDED/FAILED/CANCELLED/EXPIRED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job status monitoring\n",
    "def monitor_batch_job(job_name: str, max_polls: int = 120, poll_interval: int = 30) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Monitor batch job status until completion or timeout\n",
    "    \n",
    "    Args:\n",
    "        job_name: The batch job name to monitor\n",
    "        max_polls: Maximum number of polling attempts (default: 120 = 1 hour)\n",
    "        poll_interval: Seconds between polls (default: 30)\n",
    "    \"\"\"\n",
    "    completed_states = {\n",
    "        'JOB_STATE_SUCCEEDED',\n",
    "        'JOB_STATE_FAILED', \n",
    "        'JOB_STATE_CANCELLED',\n",
    "        'JOB_STATE_EXPIRED'\n",
    "    }\n",
    "    \n",
    "    print(f\"üëÄ Monitoring batch job: {job_name}\")\n",
    "    print(f\"‚è±Ô∏è  Polling every {poll_interval} seconds (max {max_polls} attempts)\")\n",
    "    \n",
    "    for attempt in range(1, max_polls + 1):\n",
    "        try:\n",
    "            # Get current job status\n",
    "            batch_job = client.batches.get(name=job_name)\n",
    "            \n",
    "            # Display current status\n",
    "            print(f\"\\nüìä Poll #{attempt} - Status: {batch_job.state}\")\n",
    "            \n",
    "            if hasattr(batch_job, 'request_count'):\n",
    "                print(f\"   üìù Total requests: {batch_job.request_count}\")\n",
    "                \n",
    "            if hasattr(batch_job, 'batch_stats') and batch_job.batch_stats:\n",
    "                completed = getattr(batch_job.batch_stats, 'completed_request_count', 0)\n",
    "                failed = getattr(batch_job.batch_stats, 'failed_request_count', 0)\n",
    "                print(f\"   ‚úÖ Completed: {completed}\")\n",
    "                print(f\"   ‚ùå Failed: {failed}\")\n",
    "                \n",
    "            # Check if job is complete\n",
    "            if batch_job.state in completed_states:\n",
    "                print(f\"\\nüèÅ Job finished with state: {batch_job.state}\")\n",
    "                \n",
    "                if batch_job.state == 'JOB_STATE_SUCCEEDED':\n",
    "                    print(\"üéâ Job completed successfully!\")\n",
    "                elif batch_job.state == 'JOB_STATE_FAILED':\n",
    "                    print(f\"üí• Job failed: {getattr(batch_job, 'error', 'Unknown error')}\")\n",
    "                elif batch_job.state == 'JOB_STATE_CANCELLED':\n",
    "                    print(\"üõë Job was cancelled\")\n",
    "                elif batch_job.state == 'JOB_STATE_EXPIRED':\n",
    "                    print(\"‚è∞ Job expired (ran longer than 48 hours)\")\n",
    "                \n",
    "                return {\n",
    "                    'job': batch_job,\n",
    "                    'final_state': batch_job.state,\n",
    "                    'completed': batch_job.state == 'JOB_STATE_SUCCEEDED'\n",
    "                }\n",
    "                \n",
    "            # Wait before next poll\n",
    "            if attempt < max_polls:\n",
    "                print(f\"‚è≥ Waiting {poll_interval} seconds for next poll...\")\n",
    "                time.sleep(poll_interval)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error polling job status: {e}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"‚è∞ Timeout reached after {max_polls} attempts\")\n",
    "    return None\n",
    "\n",
    "# Monitor the job if we created one\n",
    "if 'current_job_name' in locals() and current_job_name:\n",
    "    print(f\"üîÑ Starting job monitoring for: {current_job_name}\")\n",
    "    \n",
    "    # For demo purposes, let's do just a few quick polls to show the concept\n",
    "    # In practice, you'd want to poll until completion\n",
    "    demo_polls = 3\n",
    "    poll_interval = 10  # Shorter interval for demo\n",
    "    \n",
    "    print(f\"üìã Demo monitoring (first {demo_polls} polls, {poll_interval}s intervals)\")\n",
    "    \n",
    "    for i in range(1, demo_polls + 1):\n",
    "        try:\n",
    "            batch_job = client.batches.get(name=current_job_name)\n",
    "            print(f\"\\nüìä Poll #{i}:\")\n",
    "            print(f\"   State: {batch_job.state}\")\n",
    "            print(f\"   Model: {batch_job.model}\")\n",
    "            print(f\"   Created: {batch_job.create_time}\")\n",
    "            \n",
    "            if i < demo_polls:\n",
    "                print(f\"   ‚è≥ Next poll in {poll_interval}s...\")\n",
    "                time.sleep(poll_interval)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in demo poll #{i}: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nüí° For full monitoring until completion, use:\")\n",
    "    print(f\"   final_result = monitor_batch_job('{current_job_name}')\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No active job to monitor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db684b4a",
   "metadata": {},
   "source": [
    "## 7. Retrieve and Process Results\n",
    "\n",
    "Download and process batch job results once the job completes successfully. Results are returned as JSONL file for file-based jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e0fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results retrieval and processing\n",
    "def retrieve_batch_results(job_name: str) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Retrieve and parse results from a completed batch job\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the completed job\n",
    "        batch_job = client.batches.get(name=job_name)\n",
    "        \n",
    "        if batch_job.state != 'JOB_STATE_SUCCEEDED':\n",
    "            print(f\"‚ùå Job not successful. Current state: {batch_job.state}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"‚úÖ Job completed successfully!\")\n",
    "        print(f\"üìä Job details:\")\n",
    "        print(f\"   Display name: {batch_job.display_name}\")\n",
    "        print(f\"   Model: {batch_job.model}\")\n",
    "        print(f\"   State: {batch_job.state}\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Check for file-based results\n",
    "        if hasattr(batch_job, 'dest') and batch_job.dest and hasattr(batch_job.dest, 'file_name'):\n",
    "            result_file_name = batch_job.dest.file_name\n",
    "            print(f\"üìÅ Results available in file: {result_file_name}\")\n",
    "            \n",
    "            # Download the results file\n",
    "            print(f\"‚¨áÔ∏è  Downloading results file...\")\n",
    "            file_content = client.files.download(file=result_file_name)\n",
    "            \n",
    "            # Parse JSONL content\n",
    "            content_str = file_content.decode('utf-8') if isinstance(file_content, bytes) else str(file_content)\n",
    "            lines = content_str.strip().split('\\\\n')\n",
    "            \n",
    "            print(f\"üìã Processing {len(lines)} result lines...\")\n",
    "            \n",
    "            for i, line in enumerate(lines):\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        result = json.loads(line)\n",
    "                        results.append({\n",
    "                            'line_number': i + 1,\n",
    "                            'key': result.get('key', f'unknown-{i+1}'),\n",
    "                            'success': 'response' in result,\n",
    "                            'error': result.get('error'),\n",
    "                            'response_text': result.get('response', {}).get('text') if 'response' in result else None,\n",
    "                            'raw_result': result\n",
    "                        })\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"‚ö†Ô∏è  Error parsing line {i+1}: {e}\")\n",
    "                        results.append({\n",
    "                            'line_number': i + 1,\n",
    "                            'key': f'parse-error-{i+1}',\n",
    "                            'success': False,\n",
    "                            'error': f'JSON parse error: {e}',\n",
    "                            'response_text': None,\n",
    "                            'raw_result': line\n",
    "                        })\n",
    "        \n",
    "        # Check for inline results  \n",
    "        elif hasattr(batch_job, 'dest') and batch_job.dest and hasattr(batch_job.dest, 'inlined_responses'):\n",
    "            print(f\"üìÑ Results available inline\")\n",
    "            inline_responses = batch_job.dest.inlined_responses\n",
    "            \n",
    "            for i, response in enumerate(inline_responses):\n",
    "                results.append({\n",
    "                    'line_number': i + 1,\n",
    "                    'key': f'inline-{i+1}',\n",
    "                    'success': hasattr(response, 'response') and response.response,\n",
    "                    'error': getattr(response, 'error', None),\n",
    "                    'response_text': getattr(response.response, 'text', None) if hasattr(response, 'response') and response.response else None,\n",
    "                    'raw_result': response\n",
    "                })\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå No results found in job response\")\n",
    "            return None\n",
    "            \n",
    "        # Summary\n",
    "        successful = len([r for r in results if r['success']])\n",
    "        failed = len([r for r in results if not r['success']])\n",
    "        \n",
    "        print(f\"\\\\nüìà Results Summary:\")\n",
    "        print(f\"   ‚úÖ Successful: {successful}\")\n",
    "        print(f\"   ‚ùå Failed: {failed}\")\n",
    "        print(f\"   üìä Total: {len(results)}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error retrieving results: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_results_sample(results: List[Dict[str, Any]], max_samples: int = 2):\n",
    "    \"\"\"\n",
    "    Display a sample of results for review\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"üì≠ No results to display\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\\\nüîç Sample Results (showing up to {max_samples}):\")\n",
    "    \n",
    "    successful_results = [r for r in results if r['success']]\n",
    "    failed_results = [r for r in results if not r['success']]\n",
    "    \n",
    "    # Show successful results\n",
    "    for i, result in enumerate(successful_results[:max_samples]):\n",
    "        print(f\"\\\\n‚úÖ Successful Result #{i+1}:\")\n",
    "        print(f\"   Key: {result['key']}\")\n",
    "        if result['response_text']:\n",
    "            text_preview = result['response_text'][:200] + \"...\" if len(result['response_text']) > 200 else result['response_text']\n",
    "            print(f\"   Response: {text_preview}\")\n",
    "    \n",
    "    # Show failed results if any\n",
    "    if failed_results and max_samples > len(successful_results):\n",
    "        remaining_slots = max_samples - len(successful_results[:max_samples])\n",
    "        for i, result in enumerate(failed_results[:remaining_slots]):\n",
    "            print(f\"\\\\n‚ùå Failed Result #{i+1}:\")\n",
    "            print(f\"   Key: {result['key']}\")\n",
    "            print(f\"   Error: {result['error']}\")\n",
    "\n",
    "# Example usage (commented out since job may still be running)\n",
    "print(\"üí° To retrieve results after job completion:\")\n",
    "print(\"   results = retrieve_batch_results(current_job_name)\")\n",
    "print(\"   display_results_sample(results)\")\n",
    "print(\"\")\n",
    "print(\"üîß For testing with a specific job name:\")\n",
    "print(\"   # Replace 'your-job-name' with actual job name\")\n",
    "print(\"   # results = retrieve_batch_results('batches/your-job-name')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7485cae",
   "metadata": {},
   "source": [
    "## 8. Handle Inline Requests\n",
    "\n",
    "For smaller batches (< 20MB), you can submit requests directly inline without uploading files. This is more convenient for testing and smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15358039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline batch requests (for smaller datasets)\n",
    "def create_inline_batch_job(requests: List[Dict[str, Any]], display_name: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create a batch job with inline requests (no file upload needed)\n",
    "    Suitable for smaller datasets under 20MB total size\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert our JSONL format to inline format\n",
    "        inline_requests = []\n",
    "        for req in requests:\n",
    "            # Extract the actual request content\n",
    "            request_data = req['request']\n",
    "            inline_requests.append(request_data)\n",
    "        \n",
    "        print(f\"üöÄ Creating inline batch job with {len(inline_requests)} requests...\")\n",
    "        \n",
    "        # Create batch job with inline requests\n",
    "        batch_job = client.batches.create(\n",
    "            model=BATCH_CONFIG['model'],\n",
    "            src=inline_requests,  # Pass requests directly\n",
    "            config={\n",
    "                'display_name': display_name\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Inline batch job created successfully!\")\n",
    "        print(f\"   Job name: {batch_job.name}\")\n",
    "        print(f\"   Display name: {batch_job.display_name}\")\n",
    "        print(f\"   State: {batch_job.state}\")\n",
    "        print(f\"   Requests: {len(inline_requests)}\")\n",
    "        \n",
    "        return {\n",
    "            'name': batch_job.name,\n",
    "            'display_name': batch_job.display_name,\n",
    "            'state': batch_job.state,\n",
    "            'model': batch_job.model,\n",
    "            'request_count': len(inline_requests),\n",
    "            'full_job': batch_job\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating inline batch job: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create a smaller sample for inline testing\n",
    "inline_test_requests = batch_requests[:2]  # Just first 2 requests for testing\n",
    "\n",
    "print(f\"üìù Testing inline batch with {len(inline_test_requests)} requests\")\n",
    "\n",
    "# Calculate approximate size\n",
    "sample_size = len(json.dumps(inline_test_requests).encode('utf-8'))\n",
    "print(f\"üìè Estimated size: {sample_size:,} bytes ({sample_size / 1024:.1f} KB)\")\n",
    "\n",
    "if sample_size < 20 * 1024 * 1024:  # 20MB limit\n",
    "    print(\"‚úÖ Size is within inline batch limits\")\n",
    "    \n",
    "    # Create inline batch job\n",
    "    inline_job_name = f\"Inline Rug Test - {time.strftime('%Y-%m-%d %H:%M')}\"\n",
    "    inline_job_info = create_inline_batch_job(inline_test_requests, inline_job_name)\n",
    "    \n",
    "    if inline_job_info:\n",
    "        print(f\"\\\\nüéØ Inline batch job created!\")\n",
    "        print(f\"üìã Job ID: {inline_job_info['name']}\")\n",
    "        \n",
    "        # Quick status check\n",
    "        try:\n",
    "            current_job = client.batches.get(name=inline_job_info['name'])\n",
    "            print(f\"üìä Current status: {current_job.state}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not check status: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create inline batch job\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Requests too large for inline batch (>20MB)\")\n",
    "\n",
    "print(f\"\\\\nüí° Inline batches are ideal for:\")\n",
    "print(f\"   ‚Ä¢ Testing and prototyping\")\n",
    "print(f\"   ‚Ä¢ Small datasets (< 20MB)\")\n",
    "print(f\"   ‚Ä¢ Quick turnaround scenarios\")\n",
    "print(f\"   ‚Ä¢ Results returned directly in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a1fbf",
   "metadata": {},
   "source": [
    "## 9. Implement Error Handling\n",
    "\n",
    "Comprehensive error handling for batch operations, including job management, cancellation, and failure recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0976569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error handling and job management utilities\n",
    "\n",
    "def cancel_batch_job(job_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Cancel a running batch job\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üõë Cancelling batch job: {job_name}\")\n",
    "        client.batches.cancel(name=job_name)\n",
    "        \n",
    "        # Verify cancellation\n",
    "        batch_job = client.batches.get(name=job_name)\n",
    "        if batch_job.state == 'JOB_STATE_CANCELLED':\n",
    "            print(\"‚úÖ Job cancelled successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Job state after cancellation: {batch_job.state}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cancelling job: {e}\")\n",
    "        return False\n",
    "\n",
    "def delete_batch_job(job_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Delete a batch job (removes it completely)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üóëÔ∏è  Deleting batch job: {job_name}\")\n",
    "        client.batches.delete(name=job_name)\n",
    "        print(\"‚úÖ Job deleted successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error deleting job: {e}\")\n",
    "        return False\n",
    "\n",
    "def list_batch_jobs(limit: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    List recent batch jobs for monitoring and management\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üìã Listing recent batch jobs (limit: {limit})...\")\n",
    "        \n",
    "        # Note: The actual list method may vary depending on SDK version\n",
    "        # This is a conceptual implementation\n",
    "        jobs = client.batches.list(limit=limit)\n",
    "        \n",
    "        job_list = []\n",
    "        for job in jobs:\n",
    "            job_info = {\n",
    "                'name': job.name,\n",
    "                'display_name': getattr(job, 'display_name', 'N/A'),\n",
    "                'state': job.state,\n",
    "                'model': getattr(job, 'model', 'N/A'),\n",
    "                'create_time': getattr(job, 'create_time', 'N/A')\n",
    "            }\n",
    "            job_list.append(job_info)\n",
    "            \n",
    "        return job_list\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listing jobs: {e}\")\n",
    "        return []\n",
    "\n",
    "def handle_batch_errors(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze and categorize batch processing errors\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return {'error': 'No results provided'}\n",
    "    \n",
    "    error_summary = {\n",
    "        'total_requests': len(results),\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'errors_by_type': {},\n",
    "        'failed_keys': []\n",
    "    }\n",
    "    \n",
    "    for result in results:\n",
    "        if result['success']:\n",
    "            error_summary['successful'] += 1\n",
    "        else:\n",
    "            error_summary['failed'] += 1\n",
    "            error_summary['failed_keys'].append(result['key'])\n",
    "            \n",
    "            # Categorize error types\n",
    "            error_msg = str(result.get('error', 'Unknown error'))\n",
    "            error_type = 'unknown'\n",
    "            \n",
    "            if 'timeout' in error_msg.lower():\n",
    "                error_type = 'timeout'\n",
    "            elif 'quota' in error_msg.lower() or 'limit' in error_msg.lower():\n",
    "                error_type = 'quota_limit'\n",
    "            elif 'invalid' in error_msg.lower() or 'malformed' in error_msg.lower():\n",
    "                error_type = 'invalid_request'\n",
    "            elif 'permission' in error_msg.lower() or 'auth' in error_msg.lower():\n",
    "                error_type = 'auth_error'\n",
    "            \n",
    "            error_summary['errors_by_type'][error_type] = error_summary['errors_by_type'].get(error_type, 0) + 1\n",
    "    \n",
    "    # Calculate success rate\n",
    "    error_summary['success_rate'] = (error_summary['successful'] / error_summary['total_requests']) * 100\n",
    "    \n",
    "    return error_summary\n",
    "\n",
    "# Utility functions for robust batch processing\n",
    "class BatchJobManager:\n",
    "    \"\"\"\n",
    "    A helper class for managing batch jobs with error handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client, config):\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        self.active_jobs = {}\n",
    "    \n",
    "    def submit_job(self, requests_or_file, display_name, job_type='file'):\n",
    "        \"\"\"\n",
    "        Submit a batch job with error handling\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if job_type == 'file':\n",
    "                job = self.client.batches.create(\n",
    "                    model=self.config['model'],\n",
    "                    src=requests_or_file,\n",
    "                    config={'display_name': display_name}\n",
    "                )\n",
    "            else:  # inline\n",
    "                job = self.client.batches.create(\n",
    "                    model=self.config['model'],\n",
    "                    src=requests_or_file,\n",
    "                    config={'display_name': display_name}\n",
    "                )\n",
    "            \n",
    "            self.active_jobs[job.name] = {\n",
    "                'job': job,\n",
    "                'display_name': display_name,\n",
    "                'submit_time': time.time(),\n",
    "                'type': job_type\n",
    "            }\n",
    "            \n",
    "            return job.name\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to submit job: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def monitor_job(self, job_name, callback=None):\n",
    "        \"\"\"\n",
    "        Monitor a job with optional progress callback\n",
    "        \"\"\"\n",
    "        if job_name not in self.active_jobs:\n",
    "            print(f\"‚ö†Ô∏è  Job {job_name} not found in active jobs\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            job = self.client.batches.get(name=job_name)\n",
    "            \n",
    "            if callback:\n",
    "                callback(job)\n",
    "            \n",
    "            return job.state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error monitoring job {job_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def cleanup_jobs(self, older_than_hours=24):\n",
    "        \"\"\"\n",
    "        Clean up old jobs\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        cutoff_time = current_time - (older_than_hours * 3600)\n",
    "        \n",
    "        jobs_to_remove = []\n",
    "        for job_name, job_info in self.active_jobs.items():\n",
    "            if job_info['submit_time'] < cutoff_time:\n",
    "                try:\n",
    "                    self.client.batches.delete(name=job_name)\n",
    "                    jobs_to_remove.append(job_name)\n",
    "                    print(f\"üóëÔ∏è  Cleaned up old job: {job_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Could not delete old job {job_name}: {e}\")\n",
    "        \n",
    "        for job_name in jobs_to_remove:\n",
    "            del self.active_jobs[job_name]\n",
    "\n",
    "# Example usage of error handling\n",
    "print(\"üõ†Ô∏è  Error Handling Tools Available:\")\n",
    "print(\"   ‚Ä¢ cancel_batch_job(job_name)\")\n",
    "print(\"   ‚Ä¢ delete_batch_job(job_name)\")\n",
    "print(\"   ‚Ä¢ handle_batch_errors(results)\")\n",
    "print(\"   ‚Ä¢ BatchJobManager class for advanced management\")\n",
    "\n",
    "print(\"\\\\nüí° Best Practices for Error Handling:\")\n",
    "print(\"   ‚Ä¢ Always check job status before retrieving results\")\n",
    "print(\"   ‚Ä¢ Handle timeout scenarios gracefully\")\n",
    "print(\"   ‚Ä¢ Implement retry logic for transient failures\")\n",
    "print(\"   ‚Ä¢ Monitor quota usage to avoid limits\")\n",
    "print(\"   ‚Ä¢ Clean up old jobs regularly\")\n",
    "\n",
    "# Demonstrate error analysis on sample data\n",
    "sample_error_results = [\n",
    "    {'key': 'rug-1', 'success': True, 'error': None},\n",
    "    {'key': 'rug-2', 'success': False, 'error': 'Request timeout'},\n",
    "    {'key': 'rug-3', 'success': True, 'error': None},\n",
    "    {'key': 'rug-4', 'success': False, 'error': 'Quota limit exceeded'},\n",
    "]\n",
    "\n",
    "print(\"\\\\nüìä Sample Error Analysis:\")\n",
    "error_analysis = handle_batch_errors(sample_error_results)\n",
    "print(f\"   Success rate: {error_analysis['success_rate']:.1f}%\")\n",
    "print(f\"   Total requests: {error_analysis['total_requests']}\")\n",
    "print(f\"   Failed requests: {error_analysis['failed']}\")\n",
    "print(f\"   Error types: {error_analysis['errors_by_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e35b4",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated the complete Gemini Batch API workflow for processing rug analysis requests at scale with 50% cost savings.\n",
    "\n",
    "### Key Benefits of Batch API:\n",
    "- **Cost Effective**: 50% reduction in processing costs\n",
    "- **Scalable**: Handle large volumes (up to 2GB per file)\n",
    "- **Asynchronous**: Non-blocking processing with 24-hour SLA\n",
    "- **Reliable**: Built-in error handling and retry mechanisms\n",
    "\n",
    "### Integration with Next.js App:\n",
    "The techniques demonstrated here are already integrated into our rug processing application:\n",
    "\n",
    "1. **File Upload**: `src/app/api/submit-batch/route.ts`\n",
    "2. **Job Monitoring**: `src/app/api/batch-status/route.ts`  \n",
    "3. **Result Retrieval**: `src/app/api/download-results/route.ts`\n",
    "4. **Request Generation**: `src/lib/gemini-service.ts`\n",
    "\n",
    "### Production Considerations:\n",
    "- Monitor job quotas and limits\n",
    "- Implement proper error recovery\n",
    "- Set up job cleanup procedures\n",
    "- Use appropriate polling intervals\n",
    "- Handle timeout scenarios\n",
    "\n",
    "### Testing Your Implementation:\n",
    "1. Upload a CSV file through the Next.js web interface\n",
    "2. Generate batch requests with your rug data\n",
    "3. Submit the batch job and monitor progress\n",
    "4. Download and analyze results when complete\n",
    "\n",
    "The Batch API is ideal for large-scale rug inventory processing, product description generation, and bulk AI analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b25226",
   "metadata": {},
   "source": [
    "## 10. Cleanup and Reset\n",
    "\n",
    "Use this section to clean up any problematic batch jobs, reset variables, and handle error states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6677b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and Reset Tools\n",
    "\n",
    "def cleanup_notebook_state():\n",
    "    \"\"\"\n",
    "    Clean up notebook variables and reset state\n",
    "    \"\"\"\n",
    "    print(\"üßπ Cleaning up notebook state...\")\n",
    "    \n",
    "    # Clear global variables\n",
    "    globals_to_clear = [\n",
    "        'current_job_name', \n",
    "        'batch_job_info', \n",
    "        'uploaded_file_info',\n",
    "        'batch_requests',\n",
    "        'inline_job_info'\n",
    "    ]\n",
    "    \n",
    "    cleared_count = 0\n",
    "    for var_name in globals_to_clear:\n",
    "        if var_name in globals():\n",
    "            del globals()[var_name]\n",
    "            cleared_count += 1\n",
    "            print(f\"   ‚úÖ Cleared {var_name}\")\n",
    "    \n",
    "    print(f\"üéâ Cleanup complete! Cleared {cleared_count} variables\")\n",
    "    return True\n",
    "\n",
    "def safe_job_status_check(job_name: str = None):\n",
    "    \"\"\"\n",
    "    Safely check job status with error handling\n",
    "    \"\"\"\n",
    "    if not job_name:\n",
    "        if 'current_job_name' in globals():\n",
    "            job_name = current_job_name\n",
    "        else:\n",
    "            print(\"‚ùå No job name provided and no current_job_name found\")\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîç Checking status for job: {job_name}\")\n",
    "        batch_job = client.batches.get(name=job_name)\n",
    "        \n",
    "        print(f\"üìä Job Status:\")\n",
    "        print(f\"   Name: {getattr(batch_job, 'name', 'N/A')}\")\n",
    "        print(f\"   State: {getattr(batch_job, 'state', 'UNKNOWN')}\")\n",
    "        print(f\"   Display Name: {getattr(batch_job, 'display_name', 'N/A')}\")\n",
    "        print(f\"   Model: {getattr(batch_job, 'model', 'N/A')}\")\n",
    "        \n",
    "        return batch_job\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking job status: {e}\")\n",
    "        print(\"üí° This might be normal if the job was cancelled or doesn't exist\")\n",
    "        return None\n",
    "\n",
    "def force_cancel_all_jobs():\n",
    "    \"\"\"\n",
    "    Attempt to cancel any running batch jobs (use with caution)\n",
    "    \"\"\"\n",
    "    print(\"‚ö†Ô∏è  WARNING: This will attempt to cancel ALL your batch jobs!\")\n",
    "    response = input(\"Type 'YES' to confirm: \")\n",
    "    \n",
    "    if response != 'YES':\n",
    "        print(\"‚ùå Cancelled by user\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # This is a conceptual implementation - actual implementation may vary\n",
    "        print(\"üõë Attempting to list and cancel jobs...\")\n",
    "        # Note: You would need to implement actual job listing and cancellation\n",
    "        print(\"üí° Manual cancellation required - check your Google AI Studio dashboard\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during bulk cancellation: {e}\")\n",
    "        return False\n",
    "\n",
    "def reset_client_connection():\n",
    "    \"\"\"\n",
    "    Reset the Gemini API client connection\n",
    "    \"\"\"\n",
    "    global client\n",
    "    try:\n",
    "        print(\"üîÑ Resetting client connection...\")\n",
    "        \n",
    "        # Re-initialize the client\n",
    "        if 'GEMINI_API_KEY' in globals() and GEMINI_API_KEY:\n",
    "            client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "            print(\"‚úÖ Client reconnected successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå No API key available for reconnection\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error resetting client: {e}\")\n",
    "        return False\n",
    "\n",
    "# Quick cleanup options\n",
    "print(\"üõ†Ô∏è  Cleanup Tools Available:\")\n",
    "print(\"   ‚Ä¢ cleanup_notebook_state() - Clear all notebook variables\")\n",
    "print(\"   ‚Ä¢ safe_job_status_check() - Check job status safely\") \n",
    "print(\"   ‚Ä¢ reset_client_connection() - Reset API client\")\n",
    "print(\"   ‚Ä¢ force_cancel_all_jobs() - Cancel running jobs (use carefully)\")\n",
    "\n",
    "print(\"\\\\nüö® If you're seeing 'undefined batch state' errors:\")\n",
    "print(\"   1. Run: cleanup_notebook_state()\")\n",
    "print(\"   2. Run: reset_client_connection()\")\n",
    "print(\"   3. Restart from cell 1 if needed\")\n",
    "\n",
    "print(\"\\\\nüí° Quick Reset:\")\n",
    "print(\"   cleanup_notebook_state()\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
